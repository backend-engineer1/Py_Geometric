{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match same input of features\n",
    "from AttFPfeaturing import datagenerator, getdataloader\n",
    "#from Datas import dataloader\n",
    "import pandas as pd\n",
    "target_list = ['Result0']\n",
    "import pickle\n",
    "\n",
    "df = pd.read_csv('esol.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1128/1128 [00:06<00:00, 165.87it/s]\n"
     ]
    }
   ],
   "source": [
    "data = datagenerator(df, target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_loader = getdataloader(data, batch_size, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(batch=[1755], edge_attr=[3602, 10], edge_index=[2, 3602], x=[1755, 40], y=[128, 1])\n",
      "torch.Size([1755, 40])\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    print(data)\n",
    "    print(data.x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AttFP import AttentiveFP\n",
    "# generate the model architecture\n",
    "model = AttentiveFP(40, 10, 200, R= 2, T=2, dropout = 0.2, debug = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 17.57201385498047  & time: 0.2130262851715088\n",
      "train loss: 10.550328254699707  & time: 0.4110877513885498\n",
      "train loss: 8.518917878468832  & time: 0.611461877822876\n",
      "train loss: 7.07841032743454  & time: 0.798060417175293\n",
      "train loss: 6.531499528884888  & time: 0.969775915145874\n",
      "train loss: 5.941838463147481  & time: 1.13584566116333\n",
      "train loss: 5.519216912133353  & time: 1.303441047668457\n",
      "train loss: 5.161972373723984  & time: 1.4657082557678223\n",
      "train loss: 4.9315530019449  & time: 1.6442155838012695\n",
      "train loss: 2.353203058242798  & time: 0.1698307991027832\n",
      "train loss: 2.6461856365203857  & time: 0.37367701530456543\n",
      "train loss: 2.636662483215332  & time: 0.5944433212280273\n",
      "train loss: 2.47840815782547  & time: 0.8150088787078857\n",
      "train loss: 2.4401803016662598  & time: 0.997114896774292\n",
      "train loss: 2.454843203226725  & time: 1.162264108657837\n",
      "train loss: 2.5075666904449463  & time: 1.3264429569244385\n",
      "train loss: 2.4430841505527496  & time: 1.4942388534545898\n",
      "train loss: 2.3785944855804986  & time: 1.6481740474700928\n",
      "train loss: 2.288573741912842  & time: 0.1956477165222168\n",
      "train loss: 2.3955284357070923  & time: 0.3770480155944824\n",
      "train loss: 2.171616792678833  & time: 0.5904762744903564\n",
      "train loss: 2.069381594657898  & time: 0.9079341888427734\n",
      "train loss: 2.057397651672363  & time: 1.0768640041351318\n",
      "train loss: 2.0260363618532815  & time: 1.3894977569580078\n",
      "train loss: 1.9581423657281058  & time: 1.546962022781372\n",
      "train loss: 1.9710735827684402  & time: 1.717414379119873\n",
      "train loss: 1.9449226754776976  & time: 1.8837502002716064\n",
      "train loss: 1.8760958909988403  & time: 0.2006070613861084\n",
      "train loss: 2.0328161120414734  & time: 0.4369850158691406\n",
      "train loss: 1.9328252871831257  & time: 0.5938193798065186\n",
      "train loss: 1.873528629541397  & time: 0.7639575004577637\n",
      "train loss: 1.9023080110549926  & time: 0.9457950592041016\n",
      "train loss: 1.8909199833869934  & time: 1.1118552684783936\n",
      "train loss: 1.8574611800057548  & time: 1.2937240600585938\n",
      "train loss: 1.8108664751052856  & time: 1.4802064895629883\n",
      "train loss: 1.825046200278803  & time: 1.654573917388916\n",
      "train loss: 1.4426506757736206  & time: 0.19479680061340332\n",
      "train loss: 1.8300511240959167  & time: 0.41214680671691895\n",
      "train loss: 1.8578659693400066  & time: 0.5947198867797852\n",
      "train loss: 1.8227585554122925  & time: 0.7601735591888428\n",
      "train loss: 1.854633355140686  & time: 0.9243593215942383\n",
      "train loss: 1.9199875791867573  & time: 1.0939900875091553\n",
      "train loss: 1.875341807092939  & time: 1.2855346202850342\n",
      "train loss: 1.8765483349561691  & time: 1.5632884502410889\n",
      "train loss: 1.8329296484061166  & time: 2.0250394344329834\n",
      "train loss: 1.4189492464065552  & time: 0.19524717330932617\n",
      "train loss: 1.423127293586731  & time: 0.36673974990844727\n",
      "train loss: 1.4492724736531575  & time: 0.5367844104766846\n",
      "train loss: 1.463059663772583  & time: 0.7309844493865967\n",
      "train loss: 1.4795835733413696  & time: 0.898061990737915\n",
      "train loss: 1.493494172890981  & time: 1.0544195175170898\n",
      "train loss: 1.491055199078151  & time: 1.2324423789978027\n",
      "train loss: 1.5318869352340698  & time: 1.4079713821411133\n",
      "train loss: 1.5480433347377371  & time: 1.6130340099334717\n",
      "train loss: 1.228049874305725  & time: 0.21263575553894043\n",
      "train loss: 1.2859322428703308  & time: 0.4164395332336426\n",
      "train loss: 1.2333332300186157  & time: 0.5786068439483643\n",
      "train loss: 1.3687575161457062  & time: 0.7475020885467529\n",
      "train loss: 1.3673386812210082  & time: 0.9228239059448242\n",
      "train loss: 1.3764979044596355  & time: 1.0944204330444336\n",
      "train loss: 1.4060402938297816  & time: 1.2616825103759766\n",
      "train loss: 1.4118079543113708  & time: 1.452604055404663\n",
      "train loss: 1.3790326186105715  & time: 1.6207101345062256\n",
      "train loss: 1.3026988506317139  & time: 0.17715239524841309\n",
      "train loss: 1.2694497108459473  & time: 0.3485426902770996\n",
      "train loss: 1.2836877504984539  & time: 0.512519121170044\n",
      "train loss: 1.3233420848846436  & time: 0.6832475662231445\n",
      "train loss: 1.3433796405792235  & time: 0.859628438949585\n",
      "train loss: 1.3121402263641357  & time: 1.0557196140289307\n",
      "train loss: 1.3209906135286604  & time: 1.311906099319458\n",
      "train loss: 1.2915357500314713  & time: 1.5241665840148926\n",
      "train loss: 1.29533547712556  & time: 1.6886394023895264\n",
      "train loss: 1.3679252862930298  & time: 0.168534517288208\n",
      "train loss: 1.371064007282257  & time: 0.3356664180755615\n",
      "train loss: 1.3045964638392131  & time: 0.5242123603820801\n",
      "train loss: 1.3073620200157166  & time: 0.6923098564147949\n",
      "train loss: 1.3007182121276855  & time: 0.8444428443908691\n",
      "train loss: 1.2680725455284119  & time: 1.0387792587280273\n",
      "train loss: 1.2277733428137643  & time: 1.2257566452026367\n",
      "train loss: 1.219964623451233  & time: 1.5962445735931396\n",
      "train loss: 1.2253340431984434  & time: 1.7670540809631348\n",
      "train loss: 1.3133909702301025  & time: 0.17146968841552734\n",
      "train loss: 1.2954862117767334  & time: 0.3242332935333252\n",
      "train loss: 1.2015130519866943  & time: 0.4811539649963379\n",
      "train loss: 1.0944431275129318  & time: 0.6421012878417969\n",
      "train loss: 1.1072322487831117  & time: 0.7981047630310059\n",
      "train loss: 1.1297917068004608  & time: 0.976865291595459\n",
      "train loss: 1.1225129621369498  & time: 1.168473243713379\n",
      "train loss: 1.1317230239510536  & time: 1.3973157405853271\n",
      "train loss: 1.1371311448144574  & time: 1.5569007396697998\n",
      "train loss: 1.0878381729125977  & time: 0.18211627006530762\n",
      "train loss: 0.940973550081253  & time: 0.3523731231689453\n",
      "train loss: 0.9973271489143372  & time: 0.5334358215332031\n",
      "train loss: 0.9974295347929001  & time: 0.7177608013153076\n",
      "train loss: 0.9959374308586121  & time: 0.9106857776641846\n",
      "train loss: 1.0228871007760365  & time: 1.156015396118164\n",
      "train loss: 1.0086497834750585  & time: 1.325801134109497\n",
      "train loss: 0.99533461779356  & time: 1.5108060836791992\n",
      "train loss: 0.9930513577258333  & time: 1.6590795516967773\n",
      "train loss: 0.8065323829650879  & time: 0.16885757446289062\n",
      "train loss: 0.8436751067638397  & time: 0.33283257484436035\n",
      "train loss: 0.9134543140729269  & time: 0.5395040512084961\n",
      "train loss: 0.9312456995248795  & time: 0.7030167579650879\n",
      "train loss: 0.923037302494049  & time: 0.9326708316802979\n",
      "train loss: 0.9387355943520864  & time: 1.1216552257537842\n",
      "train loss: 0.9063692944390433  & time: 1.3799059391021729\n",
      "train loss: 0.8891016766428947  & time: 1.569190502166748\n",
      "train loss: 0.8818907661640898  & time: 1.8173627853393555\n",
      "train loss: 0.6219392418861389  & time: 0.3650059700012207\n",
      "train loss: 0.7910976409912109  & time: 0.5441238880157471\n",
      "train loss: 0.7346994479497274  & time: 0.7863461971282959\n",
      "train loss: 0.7979632318019867  & time: 1.012500286102295\n",
      "train loss: 0.7720935344696045  & time: 1.233168363571167\n",
      "train loss: 0.7915156682332357  & time: 1.4091532230377197\n",
      "train loss: 0.7775455287524632  & time: 1.582456111907959\n",
      "train loss: 0.7985841929912567  & time: 1.8062198162078857\n",
      "train loss: 0.7999464387589312  & time: 1.980438470840454\n",
      "train loss: 0.6244879961013794  & time: 0.1867694854736328\n",
      "train loss: 0.6831443905830383  & time: 0.37179088592529297\n",
      "train loss: 0.867838184038798  & time: 0.5523781776428223\n",
      "train loss: 0.7859590649604797  & time: 0.7593140602111816\n",
      "train loss: 0.777779495716095  & time: 0.9311330318450928\n",
      "train loss: 0.7749338845411936  & time: 1.0926239490509033\n",
      "train loss: 0.7769777519362313  & time: 1.2630579471588135\n",
      "train loss: 0.7658669874072075  & time: 1.4647495746612549\n",
      "train loss: 0.7435772279475598  & time: 1.6131713390350342\n",
      "train loss: 0.650676429271698  & time: 0.1833505630493164\n",
      "train loss: 0.6292944252490997  & time: 0.36796092987060547\n",
      "train loss: 0.5918659567832947  & time: 0.5453333854675293\n",
      "train loss: 0.6190916001796722  & time: 0.7509562969207764\n",
      "train loss: 0.6295097827911377  & time: 1.023613452911377\n",
      "train loss: 0.6156201561292013  & time: 1.3148260116577148\n",
      "train loss: 0.6269972494670323  & time: 1.598670482635498\n",
      "train loss: 0.6277418807148933  & time: 1.776395320892334\n",
      "train loss: 0.6353317693615637  & time: 2.001087188720703\n",
      "train loss: 0.5958604216575623  & time: 0.29114341735839844\n",
      "train loss: 0.5675631761550903  & time: 0.48911190032958984\n",
      "train loss: 0.5169209241867065  & time: 0.7084441184997559\n",
      "train loss: 0.5515975654125214  & time: 0.8715269565582275\n",
      "train loss: 0.5653533697128296  & time: 1.165717363357544\n",
      "train loss: 0.5659978290398916  & time: 1.6061911582946777\n",
      "train loss: 0.5939102343150547  & time: 2.1091227531433105\n",
      "train loss: 0.5883022397756577  & time: 2.295351266860962\n",
      "train loss: 0.5736541403523574  & time: 2.4729208946228027\n",
      "train loss: 0.597648561000824  & time: 0.24417805671691895\n",
      "train loss: 0.5082132071256638  & time: 0.5172994136810303\n",
      "train loss: 0.6199937959512075  & time: 1.0308773517608643\n",
      "train loss: 0.6015062257647514  & time: 1.2181990146636963\n",
      "train loss: 0.5666703164577485  & time: 1.416407823562622\n",
      "train loss: 0.6083703289429346  & time: 1.597844123840332\n",
      "train loss: 0.5953565878527505  & time: 1.7773127555847168\n",
      "train loss: 0.5845819525420666  & time: 1.9494130611419678\n",
      "train loss: 0.5884822387221857  & time: 2.2275872230529785\n",
      "train loss: 0.6031355857849121  & time: 0.25921106338500977\n",
      "train loss: 0.5605901181697845  & time: 0.4365260601043701\n",
      "train loss: 0.6193342208862305  & time: 0.6142628192901611\n",
      "train loss: 0.5890455320477486  & time: 0.7972221374511719\n",
      "train loss: 0.5589414775371552  & time: 0.9638516902923584\n",
      "train loss: 0.5464447885751724  & time: 1.1409235000610352\n",
      "train loss: 0.5448863974639347  & time: 1.311342477798462\n",
      "train loss: 0.5484956912696362  & time: 1.5008387565612793\n",
      "train loss: 0.5543981397405584  & time: 1.6843876838684082\n",
      "train loss: 0.5360100269317627  & time: 0.18593358993530273\n",
      "train loss: 0.5643064081668854  & time: 0.4666299819946289\n",
      "train loss: 0.5586034258206686  & time: 0.6458418369293213\n",
      "train loss: 0.5539112538099289  & time: 0.8175675868988037\n",
      "train loss: 0.5959744811058044  & time: 0.9994254112243652\n",
      "train loss: 0.5824550489584605  & time: 1.163684606552124\n",
      "train loss: 0.5630524797098977  & time: 1.4029796123504639\n",
      "train loss: 0.5522558689117432  & time: 1.5993363857269287\n",
      "train loss: 0.5521662159168974  & time: 1.788099765777588\n",
      "train loss: 0.5819094777107239  & time: 0.23397302627563477\n",
      "train loss: 0.5498293936252594  & time: 0.4134509563446045\n",
      "train loss: 0.5224318901697794  & time: 0.7122354507446289\n",
      "train loss: 0.5192893296480179  & time: 0.9227752685546875\n",
      "train loss: 0.5049831926822662  & time: 1.1954362392425537\n",
      "train loss: 0.5046433955430984  & time: 1.500781774520874\n",
      "train loss: 0.4871354230812618  & time: 1.7215428352355957\n",
      "train loss: 0.489877600222826  & time: 1.9266993999481201\n",
      "train loss: 0.49626739397116587  & time: 2.0902657508850098\n",
      "train loss: 0.45328038930892944  & time: 0.1679847240447998\n",
      "train loss: 0.5155352056026459  & time: 0.35025691986083984\n",
      "train loss: 0.5065996845563253  & time: 0.5699961185455322\n",
      "train loss: 0.5056223571300507  & time: 0.7665467262268066\n",
      "train loss: 0.5281377911567688  & time: 1.033686876296997\n",
      "train loss: 0.5028929809729258  & time: 1.2769644260406494\n",
      "train loss: 0.5021411946841648  & time: 1.5150818824768066\n",
      "train loss: 0.5009587965905666  & time: 1.6889674663543701\n",
      "train loss: 0.5124365564779187  & time: 1.8664486408233643\n",
      "train loss: 0.48443078994750977  & time: 0.17621302604675293\n",
      "train loss: 0.44719459116458893  & time: 0.45561671257019043\n",
      "train loss: 0.45384481549263  & time: 0.6237258911132812\n",
      "train loss: 0.47560756653547287  & time: 0.8197047710418701\n",
      "train loss: 0.48547874093055726  & time: 1.007585048675537\n",
      "train loss: 0.4970850596825282  & time: 1.2567880153656006\n",
      "train loss: 0.479938200541905  & time: 1.4263911247253418\n",
      "train loss: 0.49093130975961685  & time: 1.655540943145752\n",
      "train loss: 0.4822036032135605  & time: 1.8252322673797607\n",
      "train loss: 0.5956587791442871  & time: 0.18933463096618652\n",
      "train loss: 0.4922330230474472  & time: 0.40480661392211914\n",
      "train loss: 0.5045110881328583  & time: 0.5933070182800293\n",
      "train loss: 0.47305814921855927  & time: 0.9401354789733887\n",
      "train loss: 0.5240406513214111  & time: 1.2414827346801758\n",
      "train loss: 0.5033626904090246  & time: 1.479074478149414\n",
      "train loss: 0.5002041161060333  & time: 1.7833802700042725\n",
      "train loss: 0.5185039527714252  & time: 2.054044723510742\n",
      "train loss: 0.5069381596771538  & time: 2.340092897415161\n",
      "train loss: 0.42266371846199036  & time: 0.24535179138183594\n",
      "train loss: 0.4627826362848282  & time: 0.5988335609436035\n",
      "train loss: 0.478757510582606  & time: 0.8842685222625732\n",
      "train loss: 0.47054050862789154  & time: 1.1420526504516602\n",
      "train loss: 0.4693609356880188  & time: 1.4432213306427002\n",
      "train loss: 0.48354191581408185  & time: 1.767016887664795\n",
      "train loss: 0.4923472745077951  & time: 2.001126289367676\n",
      "train loss: 0.4944842606782913  & time: 2.188810348510742\n",
      "train loss: 0.5002790664104705  & time: 2.410271167755127\n",
      "train loss: 0.399951696395874  & time: 0.16162967681884766\n",
      "train loss: 0.49244996905326843  & time: 0.3396174907684326\n",
      "train loss: 0.45936516920725506  & time: 0.5064938068389893\n",
      "train loss: 0.44942130893468857  & time: 0.6944599151611328\n",
      "train loss: 0.4467921257019043  & time: 0.8635973930358887\n",
      "train loss: 0.44493986666202545  & time: 1.2270395755767822\n",
      "train loss: 0.4406963884830475  & time: 1.5082683563232422\n",
      "train loss: 0.46280357614159584  & time: 1.7086775302886963\n",
      "train loss: 0.45755873271759523  & time: 1.8854713439941406\n",
      "train loss: 0.49015679955482483  & time: 0.18396687507629395\n",
      "train loss: 0.46208609640598297  & time: 0.3880043029785156\n",
      "train loss: 0.5001770357290903  & time: 0.5688564777374268\n",
      "train loss: 0.5033129975199699  & time: 0.852496862411499\n",
      "train loss: 0.4914078891277313  & time: 1.0736596584320068\n",
      "train loss: 0.4734559853871663  & time: 1.3648078441619873\n",
      "train loss: 0.46067539283207487  & time: 1.6767017841339111\n",
      "train loss: 0.468557670712471  & time: 1.981748104095459\n",
      "train loss: 0.4628415737591737  & time: 2.1799395084381104\n",
      "train loss: 0.4141906499862671  & time: 0.1882469654083252\n",
      "train loss: 0.4284431040287018  & time: 0.36888813972473145\n",
      "train loss: 0.4048854907353719  & time: 0.5723881721496582\n",
      "train loss: 0.41021008789539337  & time: 0.8453285694122314\n",
      "train loss: 0.3972317695617676  & time: 1.0503642559051514\n",
      "train loss: 0.4044860651095708  & time: 1.2235233783721924\n",
      "train loss: 0.4188602438994816  & time: 1.399425745010376\n",
      "train loss: 0.42438631132245064  & time: 1.5777690410614014\n",
      "train loss: 0.4222668961007544  & time: 1.7851781845092773\n",
      "train loss: 0.37777984142303467  & time: 0.21240973472595215\n",
      "train loss: 0.3350689709186554  & time: 0.3868286609649658\n",
      "train loss: 0.3777416745821635  & time: 0.7737536430358887\n",
      "train loss: 0.3946181535720825  & time: 1.0497751235961914\n",
      "train loss: 0.4045411229133606  & time: 1.442338466644287\n",
      "train loss: 0.4134148160616557  & time: 1.9321415424346924\n",
      "train loss: 0.41720216614859446  & time: 2.1776015758514404\n",
      "train loss: 0.4119522348046303  & time: 2.3511667251586914\n",
      "train loss: 0.41381233042859017  & time: 2.5335073471069336\n",
      "train loss: 0.4639497399330139  & time: 0.20338129997253418\n",
      "train loss: 0.4339049756526947  & time: 0.44820261001586914\n",
      "train loss: 0.39367492000261944  & time: 0.6368200778961182\n",
      "train loss: 0.3952180743217468  & time: 0.9000170230865479\n",
      "train loss: 0.38976932764053346  & time: 1.1599280834197998\n",
      "train loss: 0.3894752711057663  & time: 1.4117178916931152\n",
      "train loss: 0.40510524596486774  & time: 1.6117472648620605\n",
      "train loss: 0.4097248613834381  & time: 1.779921531677246\n",
      "train loss: 0.4084560714714916  & time: 2.1827878952026367\n",
      "train loss: 0.38245320320129395  & time: 0.2602839469909668\n",
      "train loss: 0.36947739124298096  & time: 0.46633315086364746\n",
      "train loss: 0.41689862807591754  & time: 0.660616397857666\n",
      "train loss: 0.4362580478191376  & time: 0.8632862567901611\n",
      "train loss: 0.4212958037853241  & time: 1.0497572422027588\n",
      "train loss: 0.41190969447294873  & time: 1.2409121990203857\n",
      "train loss: 0.40259625230516705  & time: 1.442518949508667\n",
      "train loss: 0.3995799757540226  & time: 1.709517478942871\n",
      "train loss: 0.4157611347259359  & time: 1.926696538925171\n",
      "train loss: 0.3867711126804352  & time: 0.18615508079528809\n",
      "train loss: 0.3531225770711899  & time: 0.3771035671234131\n",
      "train loss: 0.39731188615163165  & time: 0.5619997978210449\n",
      "train loss: 0.3892275542020798  & time: 0.7468678951263428\n",
      "train loss: 0.4119973421096802  & time: 0.9526009559631348\n",
      "train loss: 0.42614272236824036  & time: 1.136171579360962\n",
      "train loss: 0.4171377420425415  & time: 1.4042272567749023\n",
      "train loss: 0.411495354026556  & time: 1.5951497554779053\n",
      "train loss: 0.40839587437345626  & time: 1.7621126174926758\n",
      "train loss: 0.41033750772476196  & time: 0.24102306365966797\n",
      "train loss: 0.42501887679100037  & time: 0.6496505737304688\n",
      "train loss: 0.3991389373938243  & time: 0.8708970546722412\n",
      "train loss: 0.4107832759618759  & time: 1.2214975357055664\n",
      "train loss: 0.39853538274765016  & time: 1.628783941268921\n",
      "train loss: 0.396123265226682  & time: 1.9651637077331543\n",
      "train loss: 0.39515233039855957  & time: 2.247725486755371\n",
      "train loss: 0.3945392072200775  & time: 2.4751718044281006\n",
      "train loss: 0.39852243196879716  & time: 2.75406813621521\n",
      "train loss: 0.28905215859413147  & time: 0.4328615665435791\n",
      "train loss: 0.33225199580192566  & time: 0.7231214046478271\n",
      "train loss: 0.35520609219868976  & time: 0.937920331954956\n",
      "train loss: 0.38499781489372253  & time: 1.137965440750122\n"
     ]
    }
   ],
   "source": [
    "# loop over data in a batch\n",
    "import time\n",
    "import torch\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "# train the network\n",
    "for t in range(200):\n",
    "    start = time.time()\n",
    "    epoch_loss =0\n",
    "    count_train = 0\n",
    "    for data in train_loader:\n",
    "        y = model(data)\n",
    "        \n",
    "        loss = loss_func(y.squeeze(), data.y.squeeze())     # must be (1. nn output, 2. target)\n",
    "\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        optimizer.step()        # apply gradients\n",
    "        count_train += data.y.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            epoch_loss += data.num_graphs * loss.item()\n",
    "    \n",
    "        stop = time.time()\n",
    "    \n",
    "        print('epoch',t+1,' - train loss:',epoch_loss/count_train, ' - runtime:',stop-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
